From 68b3b707d094d73d87f7eead47e470a458ef8b30 Mon Sep 17 00:00:00 2001
From: Ram Chandra Jangir <rjangir@codeaurora.org>
Date: Tue, 28 Mar 2017 14:00:00 +0530
Subject: [PATCH 1/5] net: qualcomm: Fix edma driver for GMAC avaiable on
 IPQ40xx

Existing edma driver is not fully functional, this change
fixes the edma driver and it add new features on top of it.

Note: This patch has heavily modified.

Signed-off-by: Ram Chandra Jangir <rjangir@codeaurora.org>
---
 drivers/net/ethernet/qualcomm/essedma/edma.c       | 396 ++++++++-----
 drivers/net/ethernet/qualcomm/essedma/edma.h       |  26 +-
 drivers/net/ethernet/qualcomm/essedma/edma_axi.c   | 647 +++++++++++++++++----
 .../net/ethernet/qualcomm/essedma/edma_ethtool.c   |  50 +-
 drivers/net/ethernet/qualcomm/essedma/ess_edma.h   |   2 +
 5 files changed, 823 insertions(+), 298 deletions(-)

diff --git a/drivers/net/ethernet/qualcomm/essedma/edma.c b/drivers/net/ethernet/qualcomm/essedma/edma.c
index 671c2c0..410d40a 100644
--- a/drivers/net/ethernet/qualcomm/essedma/edma.c
+++ b/drivers/net/ethernet/qualcomm/essedma/edma.c
@@ -23,7 +23,7 @@ bool edma_stp_rstp;
 u16 edma_ath_eth_type;

 /* edma_skb_priority_offset()
- * 	get edma skb priority
+ *	get edma skb priority
  */
 static unsigned int edma_skb_priority_offset(struct sk_buff *skb)
 {
@@ -37,14 +37,15 @@ static int edma_alloc_tx_ring(struct edma_common_info *edma_cinfo,
			      struct edma_tx_desc_ring *etdr)
 {
	struct platform_device *pdev = edma_cinfo->pdev;
+	u16 sw_size = sizeof(struct edma_sw_desc) * etdr->count;

	/* Initialize ring */
-	etdr->size = sizeof(struct edma_sw_desc) * etdr->count;
+	etdr->size = sizeof(struct edma_tx_desc) * etdr->count;
	etdr->sw_next_to_fill = 0;
	etdr->sw_next_to_clean = 0;

	/* Allocate SW descriptors */
-	etdr->sw_desc = vzalloc(etdr->size);
+	etdr->sw_desc = vzalloc(sw_size);
	if (!etdr->sw_desc) {
		dev_err(&pdev->dev, "buffer alloc of tx ring failed=%p", etdr);
		return -ENOMEM;
@@ -56,6 +57,7 @@ static int edma_alloc_tx_ring(struct edma_common_info *edma_cinfo,
	if (!etdr->hw_desc) {
		dev_err(&pdev->dev, "descriptor allocation for tx ring failed");
		vfree(etdr->sw_desc);
+		etdr->sw_desc = NULL;
		return -ENOMEM;
	}

@@ -70,12 +72,13 @@ static void edma_free_tx_ring(struct edma_common_info *edma_cinfo,
 {
	struct platform_device *pdev = edma_cinfo->pdev;

-	if (likely(etdr->dma))
+	if (likely(etdr->hw_desc)) {
		dma_free_coherent(&pdev->dev, etdr->size, etdr->hw_desc,
				 etdr->dma);

-	vfree(etdr->sw_desc);
-	etdr->sw_desc = NULL;
+		vfree(etdr->sw_desc);
+		etdr->sw_desc = NULL;
+	}
 }

 /* edma_alloc_rx_ring()
@@ -85,13 +88,14 @@ static int edma_alloc_rx_ring(struct edma_common_info *edma_cinfo,
			     struct edma_rfd_desc_ring *erxd)
 {
	struct platform_device *pdev = edma_cinfo->pdev;
+	u16 sw_size = sizeof(struct edma_sw_desc) * erxd->count;

	erxd->size = sizeof(struct edma_sw_desc) * erxd->count;
	erxd->sw_next_to_fill = 0;
	erxd->sw_next_to_clean = 0;

	/* Allocate SW descriptors */
-	erxd->sw_desc = vzalloc(erxd->size);
+	erxd->sw_desc = vzalloc(sw_size);
	if (!erxd->sw_desc)
		return -ENOMEM;

@@ -100,6 +104,7 @@ static int edma_alloc_rx_ring(struct edma_common_info *edma_cinfo,
			GFP_KERNEL);
	if (!erxd->hw_desc) {
		vfree(erxd->sw_desc);
+		erxd->sw_desc = NULL;
		return -ENOMEM;
	}

@@ -110,16 +115,17 @@ static int edma_alloc_rx_ring(struct edma_common_info *edma_cinfo,
  *	Free rx ring allocated by alloc_rx_ring
  */
 static void edma_free_rx_ring(struct edma_common_info *edma_cinfo,
-			     struct edma_rfd_desc_ring *rxdr)
+			     struct edma_rfd_desc_ring *erxd)
 {
	struct platform_device *pdev = edma_cinfo->pdev;

-	if (likely(rxdr->dma))
-		dma_free_coherent(&pdev->dev, rxdr->size, rxdr->hw_desc,
-				 rxdr->dma);
+	if (likely(erxd->hw_desc)) {
+		dma_free_coherent(&pdev->dev, erxd->size, erxd->hw_desc,
+				 erxd->dma);

-	vfree(rxdr->sw_desc);
-	rxdr->sw_desc = NULL;
+		vfree(erxd->sw_desc);
+		erxd->sw_desc = NULL;
+	}
 }

 /* edma_configure_tx()
@@ -135,7 +141,6 @@ static void edma_configure_tx(struct edma_common_info *edma_cinfo)
	edma_write_reg(EDMA_REG_TXQ_CTRL, txq_ctrl_data);
 }

-
 /* edma_configure_rx()
  *	configure reception control data
  */
@@ -197,19 +202,43 @@ static int edma_alloc_rx_buf(struct edma_common_info

		if (sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_REUSE) {
			skb = sw_desc->skb;
+
+			/* Clear REUSE flag */
+			sw_desc->flags &= ~EDMA_SW_DESC_FLAG_SKB_REUSE;
		} else {
			/* alloc skb */
			skb = netdev_alloc_skb(edma_netdev[0], length);
			if (!skb) {
				/* Better luck next round */
+				sw_desc->flags = 0;
				break;
			}
		}

-		if (edma_cinfo->page_mode) {
+		if (!edma_cinfo->page_mode) {
+			sw_desc->dma = dma_map_single(&pdev->dev, skb->data,
+						     length, DMA_FROM_DEVICE);
+				if (dma_mapping_error(&pdev->dev, sw_desc->dma)) {
+					WARN_ONCE(0, "EDMA DMA mapping failed for linear address %x", sw_desc->dma);
+					sw_desc->flags = 0;
+					sw_desc->skb = NULL;
+					dev_kfree_skb_any(skb);
+					break;
+				}
+
+				/*
+				 * We should not exit from here with REUSE flag set
+				 * This is to avoid re-using same sk_buff for next
+				 * time around
+				 */
+				sw_desc->flags = EDMA_SW_DESC_FLAG_SKB_HEAD;
+				sw_desc->length = length;
+		} else {
			struct page *pg = alloc_page(GFP_ATOMIC);

			if (!pg) {
+				sw_desc->flags = 0;
+				sw_desc->skb = NULL;
				dev_kfree_skb_any(skb);
				break;
			}
@@ -217,8 +246,10 @@ static int edma_alloc_rx_buf(struct edma_common_info
			sw_desc->dma = dma_map_page(&pdev->dev, pg, 0,
						   edma_cinfo->rx_page_buffer_len,
						   DMA_FROM_DEVICE);
-			if (dma_mapping_error(&pdev->dev,
-				    sw_desc->dma)) {
+			if (dma_mapping_error(&pdev->dev, sw_desc->dma)) {
+				WARN_ONCE(0, "EDMA DMA mapping failed for page address %x", sw_desc->dma);
+				sw_desc->flags = 0;
+				sw_desc->skb = NULL;
				__free_page(pg);
				dev_kfree_skb_any(skb);
				break;
@@ -228,22 +259,11 @@ static int edma_alloc_rx_buf(struct edma_common_info
					   edma_cinfo->rx_page_buffer_len);
			sw_desc->flags = EDMA_SW_DESC_FLAG_SKB_FRAG;
			sw_desc->length = edma_cinfo->rx_page_buffer_len;
-		} else {
-			sw_desc->dma = dma_map_single(&pdev->dev, skb->data,
-						     length, DMA_FROM_DEVICE);
-			if (dma_mapping_error(&pdev->dev,
-			   sw_desc->dma)) {
-				dev_kfree_skb_any(skb);
-				break;
-			}
-
-			sw_desc->flags = EDMA_SW_DESC_FLAG_SKB_HEAD;
-			sw_desc->length = length;
		}

		/* Update the buffer info */
		sw_desc->skb = skb;
-		rx_desc = (&((struct edma_rx_free_desc *)(erdr->hw_desc))[i]);
+		rx_desc = (&(erdr->hw_desc)[i]);
		rx_desc->buffer_addr = cpu_to_le64(sw_desc->dma);
		if (++i == erdr->count)
			i = 0;
@@ -307,7 +327,7 @@ static void edma_init_desc(struct edma_common_info *edma_cinfo)
		rfd_ring = edma_cinfo->rfd_ring[j];
		/* Update Receive Free descriptor ring base address */
		edma_write_reg(EDMA_REG_RFD_BASE_ADDR_Q(j),
-			(u32)(rfd_ring->dma));
+			      (u32)(rfd_ring->dma));
		j += ((edma_cinfo->num_rx_queues == 4) ? 2 : 1);
	}

@@ -337,7 +357,7 @@ static void edma_init_desc(struct edma_common_info *edma_cinfo)
  *	Api to check checksum on receive packets
  */
 static void edma_receive_checksum(struct edma_rx_return_desc *rd,
-						 struct sk_buff *skb)
+				 struct sk_buff *skb)
 {
	skb_checksum_none_assert(skb);

@@ -354,23 +374,36 @@ static void edma_receive_checksum(struct edma_rx_return_desc *rd,
 /* edma_clean_rfd()
  *	clean up rx resourcers on error
  */
-static void edma_clean_rfd(struct edma_rfd_desc_ring *erdr, u16 index)
+static void edma_clean_rfd(struct platform_device *pdev,
+			  struct edma_rfd_desc_ring *erdr,
+			  u16 index,
+			  int pos)
 {
-	struct edma_rx_free_desc *rx_desc;
-	struct edma_sw_desc *sw_desc;
+	struct edma_rx_free_desc *rx_desc = &(erdr->hw_desc[index]);
+	struct edma_sw_desc *sw_desc = &erdr->sw_desc[index];
+
+	/* Unmap non-first RFD positions in packet */
+	if (pos) {
+		if (likely(sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_HEAD))
+			dma_unmap_single(&pdev->dev, sw_desc->dma,
+					sw_desc->length, DMA_FROM_DEVICE);
+		else
+			dma_unmap_page(&pdev->dev, sw_desc->dma,
+				      sw_desc->length, DMA_FROM_DEVICE);
+	}

-	rx_desc = (&((struct edma_rx_free_desc *)(erdr->hw_desc))[index]);
-	sw_desc = &erdr->sw_desc[index];
	if (sw_desc->skb) {
		dev_kfree_skb_any(sw_desc->skb);
		sw_desc->skb = NULL;
	}

+	sw_desc->flags = 0;
	memset(rx_desc, 0, sizeof(struct edma_rx_free_desc));
 }

-/* edma_rx_complete_fraglist()
- *	Complete Rx processing for fraglist skbs
+/*
+ * edma_rx_complete_stp_rstp()
+ *	Complete Rx processing for STP RSTP packets
  */
 static void edma_rx_complete_stp_rstp(struct sk_buff *skb, int port_id, struct edma_rx_return_desc *rd)
 {
@@ -380,7 +413,7 @@ static void edma_rx_complete_stp_rstp(struct sk_buff *skb, int port_id, struct e
	u8 mac_addr[EDMA_ETH_HDR_LEN];

	port_type = (rd->rrd1 >> EDMA_RRD_PORT_TYPE_SHIFT)
-		                & EDMA_RRD_PORT_TYPE_MASK;
+				& EDMA_RRD_PORT_TYPE_MASK;
	/* if port type is 0x4, then only proceed with
	 * other stp/rstp calculation
	 */
@@ -416,7 +449,7 @@ static void edma_rx_complete_stp_rstp(struct sk_buff *skb, int port_id, struct e
  *	Complete Rx processing for fraglist skbs
  */
 static int edma_rx_complete_fraglist(struct sk_buff *skb, u16 num_rfds, u16 length, u32 sw_next_to_clean,
-					u16 *cleaned_count, struct edma_rfd_desc_ring *erdr, struct edma_common_info *edma_cinfo)
+				    struct edma_rfd_desc_ring *erdr, struct edma_common_info *edma_cinfo)
 {
	struct platform_device *pdev = edma_cinfo->pdev;
	struct edma_hw *hw = &edma_cinfo->hw;
@@ -433,6 +466,7 @@ static int edma_rx_complete_fraglist(struct sk_buff *skb, u16 num_rfds, u16 leng
	/* clean-up all related sw_descs */
	for (i = 1; i < num_rfds; i++) {
		struct sk_buff *skb_prev;
+
		sw_desc = &erdr->sw_desc[sw_next_to_clean];
		skb_temp = sw_desc->skb;

@@ -461,7 +495,6 @@ static int edma_rx_complete_fraglist(struct sk_buff *skb, u16 num_rfds, u16 leng

		/* Increment SW index */
		sw_next_to_clean = (sw_next_to_clean + 1) & (erdr->count - 1);
-		(*cleaned_count)++;
	}

	return sw_next_to_clean;
@@ -470,8 +503,10 @@ static int edma_rx_complete_fraglist(struct sk_buff *skb, u16 num_rfds, u16 leng
 /* edma_rx_complete_paged()
  *	Complete Rx processing for paged skbs
  */
-static int edma_rx_complete_paged(struct sk_buff *skb, u16 num_rfds, u16 length, u32 sw_next_to_clean,
-					u16 *cleaned_count, struct edma_rfd_desc_ring *erdr, struct edma_common_info *edma_cinfo)
+static int edma_rx_complete_paged(struct sk_buff *skb, u16 num_rfds,
+				 u16 length, u32 sw_next_to_clean,
+				struct edma_rfd_desc_ring *erdr,
+				struct edma_common_info *edma_cinfo)
 {
	struct platform_device *pdev = edma_cinfo->pdev;
	struct sk_buff *skb_temp;
@@ -504,14 +539,15 @@ static int edma_rx_complete_paged(struct sk_buff *skb, u16 num_rfds, u16 length,
			skb_temp = sw_desc->skb;
			frag = &skb_shinfo(skb_temp)->frags[0];
			dma_unmap_page(&pdev->dev, sw_desc->dma,
-				sw_desc->length, DMA_FROM_DEVICE);
+				      sw_desc->length, DMA_FROM_DEVICE);

			if (size_remaining < edma_cinfo->rx_page_buffer_len)
				frag->size = size_remaining;

			skb_fill_page_desc(skb, i, skb_frag_page(frag),
-					0, frag->size);
+					  0, frag->size);

+			/* We used frag pages from skb_temp in skb */
			skb_shinfo(skb_temp)->nr_frags = 0;
			dev_kfree_skb_any(skb_temp);

@@ -521,7 +557,6 @@ static int edma_rx_complete_paged(struct sk_buff *skb, u16 num_rfds, u16 length,

			/* Increment SW index */
			sw_next_to_clean = (sw_next_to_clean + 1) & (erdr->count - 1);
-			(*cleaned_count)++;
		}
	}

@@ -538,17 +573,9 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,
 {
	struct platform_device *pdev = edma_cinfo->pdev;
	struct edma_rfd_desc_ring *erdr = edma_cinfo->rfd_ring[queue_id];
-	struct net_device *netdev;
-	struct edma_adapter *adapter;
-	struct edma_sw_desc *sw_desc;
-	struct sk_buff *skb;
-	struct edma_rx_return_desc *rd;
	u16 hash_type, rrd[8], cleaned_count = 0, length = 0, num_rfds = 1,
	    sw_next_to_clean, hw_next_to_clean = 0, vlan = 0, ret_count = 0;
	u32 data = 0;
-	u8 *vaddr;
-	int port_id, i, drop_count = 0;
-	u32 priority;
	u16 count = erdr->count, rfd_avail;
	u8 queue_to_rxid[8] = {0, 0, 1, 1, 2, 2, 3, 3};

@@ -556,37 +583,44 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,

	edma_read_reg(EDMA_REG_RFD_IDX_Q(queue_id), &data);
	hw_next_to_clean = (data >> EDMA_RFD_CONS_IDX_SHIFT) &
-			   EDMA_RFD_CONS_IDX_MASK;
+			EDMA_RFD_CONS_IDX_MASK;

	do {
		while (sw_next_to_clean != hw_next_to_clean) {
+			struct net_device *netdev;
+			struct edma_adapter *adapter;
+			struct edma_sw_desc *sw_desc;
+			struct sk_buff *skb;
+			struct edma_rx_return_desc *rd;
+			u8 *vaddr;
+			int port_id, i, drop_count = 0;
+			u32 priority;
+
			if (!work_to_do)
				break;

			sw_desc = &erdr->sw_desc[sw_next_to_clean];
			skb = sw_desc->skb;

-			/* Unmap the allocated buffer */
-			if (likely(sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_HEAD))
+			/* Get RRD */
+			if (!edma_cinfo->page_mode) {
				dma_unmap_single(&pdev->dev, sw_desc->dma,
-					        sw_desc->length, DMA_FROM_DEVICE);
-			else
+						sw_desc->length, DMA_FROM_DEVICE);
+				rd = (struct edma_rx_return_desc *)skb->data;
+
+			} else {
				dma_unmap_page(&pdev->dev, sw_desc->dma,
					      sw_desc->length, DMA_FROM_DEVICE);
-
-			/* Get RRD */
-			if (edma_cinfo->page_mode) {
				vaddr = kmap_atomic(skb_frag_page(&skb_shinfo(skb)->frags[0]));
				memcpy((uint8_t *)&rrd[0], vaddr, 16);
				rd = (struct edma_rx_return_desc *)rrd;
				kunmap_atomic(vaddr);
-			} else {
-				rd = (struct edma_rx_return_desc *)skb->data;
			}

			/* Check if RRD is valid */
			if (!(rd->rrd7 & EDMA_RRD_DESC_VALID)) {
-				edma_clean_rfd(erdr, sw_next_to_clean);
+				dev_err(&pdev->dev, "Incorrect RRD DESC valid bit set");
+				edma_clean_rfd(pdev, erdr, sw_next_to_clean, 0);
				sw_next_to_clean = (sw_next_to_clean + 1) &
						   (erdr->count - 1);
				cleaned_count++;
@@ -599,25 +633,43 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,
			/* Get Rx port ID from switch */
			port_id = (rd->rrd1 >> EDMA_PORT_ID_SHIFT) & EDMA_PORT_ID_MASK;
			if (!port_id) {
-				dev_err(&pdev->dev, "Invalid RRD source port bit set");
+				if (net_ratelimit()) {
+					dev_err(&pdev->dev,
+						"Incorrect RRD sport bit set");
+					dev_err(&pdev->dev, "RRD Dump\n rrd0:%x rrd1: %x rrd2:%x rrd3: %x rrd4: %x rrd5: %x rrd6: %x rrd7: %x",
+						rd->rrd0, rd->rrd1, rd->rrd2,
+						rd->rrd3, rd->rrd4, rd->rrd5,
+						rd->rrd6, rd->rrd7);
+					dev_err(&pdev->dev, "Num_rfds: %d src_port: %d, pkt_size: %d cvlan_tag: %d\n",
+						num_rfds,
+						rd->rrd1 &
+						EDMA_RRD_SRC_PORT_NUM_MASK,
+						rd->rrd6 &
+						EDMA_RRD_PKT_SIZE_MASK,
+						rd->rrd7 & EDMA_RRD_CVLAN);
+				}
				for (i = 0; i < num_rfds; i++) {
-					edma_clean_rfd(erdr, sw_next_to_clean);
+					edma_clean_rfd(pdev, erdr, sw_next_to_clean, i);
					sw_next_to_clean = (sw_next_to_clean + 1) & (erdr->count - 1);
-					cleaned_count++;
				}
+
+				cleaned_count += num_rfds;
				continue;
			}

			/* check if we have a sink for the data we receive.
			 * If the interface isn't setup, we have to drop the
			 * incoming data for now.
			 */
			netdev = edma_cinfo->netdev[0];
			if (!netdev) {
-				edma_clean_rfd(erdr, sw_next_to_clean);
-				sw_next_to_clean = (sw_next_to_clean + 1) &
-						   (erdr->count - 1);
-				cleaned_count++;
+				dev_err(&pdev->dev, "Invalid netdev");
+				for (i = 0; i < num_rfds; i++) {
+					edma_clean_rfd(pdev, erdr, sw_next_to_clean, i);
+					sw_next_to_clean = (sw_next_to_clean + 1) & (erdr->count - 1);
+				}
+
+				cleaned_count += num_rfds;
				continue;
			}
			adapter = netdev_priv(netdev);
@@ -640,7 +688,7 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,
			if (likely(!priority && !edma_cinfo->page_mode && (num_rfds <= 1))) {
				rfd_avail = (count + sw_next_to_clean - hw_next_to_clean - 1) & (count - 1);
				if (rfd_avail < EDMA_RFD_AVAIL_THR) {
-					sw_desc->flags = EDMA_SW_DESC_FLAG_SKB_REUSE;
+					sw_desc->flags |= EDMA_SW_DESC_FLAG_SKB_REUSE;
					sw_next_to_clean = (sw_next_to_clean + 1) & (erdr->count - 1);
					adapter->stats.rx_dropped++;
					cleaned_count++;
@@ -668,15 +716,16 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,
			sw_next_to_clean = (sw_next_to_clean + 1) &
					   (erdr->count - 1);

-			cleaned_count++;
-
			/* Get the packet size and allocate buffer */
			length = rd->rrd6 & EDMA_RRD_PKT_SIZE_MASK;

			if (edma_cinfo->page_mode) {
				/* paged skb */
-				sw_next_to_clean = edma_rx_complete_paged(skb, num_rfds, length, sw_next_to_clean, &cleaned_count, erdr, edma_cinfo);
+				sw_next_to_clean = edma_rx_complete_paged(skb, num_rfds, length,
+									 sw_next_to_clean,
+									 erdr, edma_cinfo);
				if (!pskb_may_pull(skb, ETH_HLEN)) {
+					cleaned_count += num_rfds;
					dev_kfree_skb_any(skb);
					continue;
				}
@@ -688,16 +737,18 @@ static void edma_rx_complete(struct edma_common_info *edma_cinfo,
				 * starts from an offset of 16.
				 */
				skb_reserve(skb, 16);
-				if (likely((num_rfds <= 1) || !edma_cinfo->fraglist_mode)) {
+				if (likely((num_rfds <= 1) || !edma_cinfo->fraglist_mode))
					skb_put(skb, length);
-				} else {
-					sw_next_to_clean = edma_rx_complete_fraglist(skb, num_rfds, length, sw_next_to_clean, &cleaned_count, erdr, edma_cinfo);
-				}
+				else
+					sw_next_to_clean = edma_rx_complete_fraglist(skb, num_rfds, length,
+										    sw_next_to_clean,
+										    erdr, edma_cinfo);
			}

-			if (edma_stp_rstp) {
+			cleaned_count += num_rfds;
+
+			if (edma_stp_rstp)
				edma_rx_complete_stp_rstp(skb, port_id, rd);
-			}

			skb->protocol = eth_type_trans(skb, netdev);

@@ -769,13 +820,14 @@ static int edma_delete_rfs_filter(struct edma_adapter *adapter,
 {
	int res = -1;

-	struct flow_keys *keys = &filter_node->keys;
-
	if (likely(adapter->set_rfs_rule))
-		res = (*adapter->set_rfs_rule)(adapter->netdev,
-			flow_get_u32_src(keys), flow_get_u32_dst(keys),
-			keys->ports.src, keys->ports.dst,
-			keys->basic.ip_proto, filter_node->rq_id, 0);
+		res  =  (*adapter->set_rfs_rule)(adapter->netdev,
+				filter_node->keys.addrs.v4addrs.src,
+				filter_node->keys.addrs.v4addrs.dst, filter_node->keys.ports.src,
+				filter_node->keys.ports.dst,
+				filter_node->keys.basic.ip_proto,
+				filter_node->rq_id,
+				0);

	return res;
 }
@@ -784,25 +836,20 @@ static int edma_delete_rfs_filter(struct edma_adapter *adapter,
  *	Add RFS filter to switch
  */
 static int edma_add_rfs_filter(struct edma_adapter *adapter,
-			       struct flow_keys *keys, u16 rq,
-			       struct edma_rfs_filter_node *filter_node)
+			      struct flow_keys *keys, u16 rq,
+			      struct edma_rfs_filter_node *filter_node)
 {
	int res = -1;

-	struct flow_keys *dest_keys = &filter_node->keys;
+	filter_node->keys.addrs.v4addrs.src = keys->addrs.v4addrs.src;
+	filter_node->keys.addrs.v4addrs.dst = keys->addrs.v4addrs.dst;
+	filter_node->keys.ports.ports = keys->ports.ports;
+	filter_node->keys.basic.ip_proto = keys->basic.ip_proto;

-	memcpy(dest_keys, &filter_node->keys, sizeof(*dest_keys));
-/*
-	dest_keys->control = keys->control;
-	dest_keys->basic = keys->basic;
-	dest_keys->addrs = keys->addrs;
-	dest_keys->ports = keys->ports;
-	dest_keys.ip_proto = keys->ip_proto;
-*/
	/* Call callback registered by ESS driver */
	if (likely(adapter->set_rfs_rule))
-		res = (*adapter->set_rfs_rule)(adapter->netdev, flow_get_u32_src(keys),
-		      flow_get_u32_dst(keys), keys->ports.src, keys->ports.dst,
+		res = (*adapter->set_rfs_rule)(adapter->netdev, keys->addrs.v4addrs.src,
+		      keys->addrs.v4addrs.dst, keys->ports.src, keys->ports.dst,
		      keys->basic.ip_proto, rq, 1);

	return res;
@@ -817,17 +864,16 @@ static struct edma_rfs_filter_node *edma_rfs_key_search(struct hlist_head *h,
	struct edma_rfs_filter_node *p;

	hlist_for_each_entry(p, h, node)
-		if (flow_get_u32_src(&p->keys) == flow_get_u32_src(key) &&
-		    flow_get_u32_dst(&p->keys) == flow_get_u32_dst(key) &&
-		    p->keys.ports.src == key->ports.src &&
-		    p->keys.ports.dst == key->ports.dst &&
-		    p->keys.basic.ip_proto == key->basic.ip_proto)
+		if (p->keys.addrs.v4addrs.src == key->addrs.v4addrs.src &&
+		   p->keys.addrs.v4addrs.dst == key->addrs.v4addrs.dst &&
+		   p->keys.ports.ports == key->ports.ports &&
+		   p->keys.basic.ip_proto == key->basic.ip_proto)
			return p;
	return NULL;
 }

 /* edma_initialise_rfs_flow_table()
- * 	Initialise EDMA RFS flow table
+ *	Initialise EDMA RFS flow table
  */
 static void edma_initialise_rfs_flow_table(struct edma_adapter *adapter)
 {
@@ -851,7 +897,7 @@ static void edma_initialise_rfs_flow_table(struct edma_adapter *adapter)
 }

 /* edma_free_rfs_flow_table()
- * 	Free EDMA RFS flow table
+ *	Free EDMA RFS flow table
  */
 static void edma_free_rfs_flow_table(struct edma_adapter *adapter)
 {
@@ -873,11 +919,11 @@ static void edma_free_rfs_flow_table(struct edma_adapter *adapter)

		hhead = &adapter->rfs.hlist_head[i];
		hlist_for_each_entry_safe(filter_node, tmp, hhead, node) {
-			res  = edma_delete_rfs_filter(adapter, filter_node);
+			res = edma_delete_rfs_filter(adapter, filter_node);
			if (res < 0)
				dev_warn(&adapter->netdev->dev,
					"EDMA going down but RFS entry %d not allowed to be flushed by Switch",
-				        filter_node->flow_id);
+					filter_node->flow_id);
			hlist_del(&filter_node->node);
			kfree(filter_node);
		}
@@ -894,14 +940,14 @@ static inline void edma_tx_unmap_and_free(struct platform_device *pdev,
	struct sk_buff *skb = sw_desc->skb;

	if (likely((sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_HEAD) ||
-			(sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_FRAGLIST)))
+		  (sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_FRAGLIST)))
		/* unmap_single for skb head area */
		dma_unmap_single(&pdev->dev, sw_desc->dma,
				sw_desc->length, DMA_TO_DEVICE);
	else if (sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_FRAG)
		/* unmap page for paged fragments */
		dma_unmap_page(&pdev->dev, sw_desc->dma,
-		  	      sw_desc->length, DMA_TO_DEVICE);
+				sw_desc->length, DMA_TO_DEVICE);

	if (likely(sw_desc->flags & EDMA_SW_DESC_FLAG_LAST))
		dev_kfree_skb_any(skb);
@@ -954,6 +1000,7 @@ static struct edma_sw_desc *edma_get_tx_buffer(struct edma_common_info *edma_cin
					       struct edma_tx_desc *tpd, int queue_id)
 {
	struct edma_tx_desc_ring *etdr = edma_cinfo->tpd_ring[queue_id];
+
	return &etdr->sw_desc[tpd - (struct edma_tx_desc *)etdr->hw_desc];
 }

@@ -1010,7 +1057,7 @@ static inline int edma_tx_queue_get(struct edma_adapter *adapter,
  *	update the producer index for the ring transmitted
  */
 static void edma_tx_update_hw_idx(struct edma_common_info *edma_cinfo,
-			         struct sk_buff *skb, int queue_id)
+				struct sk_buff *skb, int queue_id)
 {
	struct edma_tx_desc_ring *etdr = edma_cinfo->tpd_ring[queue_id];
	u32 tpd_idx_data;
@@ -1056,9 +1103,11 @@ static void edma_rollback_tx(struct edma_adapter *adapter,
  * gets mapped
  */
 static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
-			       struct edma_adapter *adapter, struct sk_buff *skb, int queue_id,
-			       unsigned int flags_transmit, u16 from_cpu, u16 dp_bitmap,
-			       bool packet_is_rstp, int nr_frags)
+				struct edma_adapter *adapter,
+				struct sk_buff *skb, int queue_id,
+				unsigned int flags_transmit,
+				u16 from_cpu, u16 dp_bitmap,
+				bool packet_is_rstp, int nr_frags)
 {
	struct edma_sw_desc *sw_desc = NULL;
	struct platform_device *pdev = edma_cinfo->pdev;
@@ -1068,9 +1117,6 @@ static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
	u32 word1 = 0, word3 = 0, lso_word1 = 0, svlan_tag = 0;
	u16 buf_len, lso_desc_len = 0;

-	/* It should either be a nr_frags skb or fraglist skb but not both */
-	BUG_ON(nr_frags && skb_has_frag_list(skb));
-
	if (skb_is_gso(skb)) {
		/* TODO: What additional checks need to be performed here */
		if (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {
@@ -1091,7 +1137,7 @@ static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
	} else if (flags_transmit & EDMA_HW_CHECKSUM) {
			u8 css, cso;
			cso = skb_checksum_start_offset(skb);
-			css = cso  + skb->csum_offset;
+			css = cso + skb->csum_offset;

			word1 |= (EDMA_TPD_CUSTOM_CSUM_EN);
			word1 |= (cso >> 1) << EDMA_TPD_HDR_SHIFT;
@@ -1102,7 +1148,7 @@ static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
		word1 |= EDMA_TPD_PPPOE_EN;

	if (flags_transmit & EDMA_VLAN_TX_TAG_INSERT_FLAG) {
-		switch(skb->vlan_proto) {
+		switch (skb->vlan_proto) {
		case htons(ETH_P_8021Q):
			word3 |= (1 << EDMA_TX_INS_CVLAN);
			word3 |= skb_vlan_tag_get(skb) << EDMA_TX_CVLAN_TAG_SHIFT;
@@ -1204,6 +1250,8 @@ static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
		tpd->word3 = word3;
	}

+	i = 0;
+
	/* Walk through all paged fragments */
	while (nr_frags--) {
		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
@@ -1245,10 +1293,39 @@ static int edma_tx_map_and_fill(struct edma_common_info *edma_cinfo,
		tpd->word1 = word1 | lso_word1;
		tpd->word3 = word3;
		sw_desc->flags |= EDMA_SW_DESC_FLAG_SKB_FRAGLIST;
+
+		i = 0;
+
+		nr_frags = skb_shinfo(iter_skb)->nr_frags;
+
+		/* Walk through paged frags for this fraglist skb */
+		while (nr_frags--) {
+			skb_frag_t *frag = &skb_shinfo(iter_skb)->frags[i];
+			buf_len = skb_frag_size(frag);
+			tpd = edma_get_next_tpd(edma_cinfo, queue_id);
+			sw_desc = edma_get_tx_buffer(edma_cinfo, tpd, queue_id);
+			sw_desc->length = buf_len;
+			sw_desc->flags |= EDMA_SW_DESC_FLAG_SKB_FRAG;
+
+			sw_desc->dma = skb_frag_dma_map(&pdev->dev, frag,
+					0, buf_len, DMA_TO_DEVICE);
+			if (dma_mapping_error(NULL, sw_desc->dma))
+				goto dma_error;
+
+			tpd->addr = cpu_to_le32(sw_desc->dma);
+			tpd->len  = cpu_to_le16(buf_len);
+			tpd->svlan_tag = svlan_tag;
+			tpd->word1 = word1 | lso_word1;
+			tpd->word3 = word3;
+			i++;
+		}
	}

-	if (tpd)
-		tpd->word1 |= 1 << EDMA_TPD_EOP_SHIFT;
+	/* If tpd or sw_desc is still unitiialized then we need to return */
+	if ((!tpd) || (!sw_desc))
+		return -EINVAL;
+
+	tpd->word1 |= 1 << EDMA_TPD_EOP_SHIFT;

	sw_desc->skb = skb;
	sw_desc->flags |= EDMA_SW_DESC_FLAG_LAST;
@@ -1325,20 +1402,25 @@ netdev_tx_t edma_xmit(struct sk_buff *skb,
	struct edma_adapter *adapter = netdev_priv(net_dev);
	struct edma_common_info *edma_cinfo = adapter->edma_cinfo;
	struct edma_tx_desc_ring *etdr;
-	u16 from_cpu, dp_bitmap, txq_id;
-	int ret, nr_frags = 0, num_tpds_needed = 1, queue_id;
+	u16 from_cpu = 0, dp_bitmap = 0, txq_id;
+	int ret, nr_frags_first = 0, num_tpds_needed = 1, queue_id = 0;
	unsigned int flags_transmit = 0;
	bool packet_is_rstp = false;
	struct netdev_queue *nq = NULL;

	if (skb_shinfo(skb)->nr_frags) {
-		nr_frags = skb_shinfo(skb)->nr_frags;
-		num_tpds_needed += nr_frags;
-	} else if (skb_has_frag_list(skb)) {
+		nr_frags_first = skb_shinfo(skb)->nr_frags;
+		num_tpds_needed += nr_frags_first;
+	}
+
+	if (skb_has_frag_list(skb)) {
		struct sk_buff *iter_skb;

-		skb_walk_frags(skb, iter_skb)
-			num_tpds_needed++;
+		/* Walk through fraglist skbs making a note of nr_frags */
+		skb_walk_frags(skb, iter_skb) {
+			/* One TPD for skb->data and more for nr_frags */
+			num_tpds_needed += (1 + skb_shinfo(iter_skb)->nr_frags);
+		}
	}

	if (num_tpds_needed > EDMA_MAX_SKB_FRAGS) {
@@ -1399,7 +1481,8 @@ netdev_tx_t edma_xmit(struct sk_buff *skb,

	/* Map and fill descriptor for Tx */
	ret = edma_tx_map_and_fill(edma_cinfo, adapter, skb, queue_id,
-		flags_transmit, from_cpu, dp_bitmap, packet_is_rstp, nr_frags);
+		flags_transmit, from_cpu, dp_bitmap,
+		packet_is_rstp, nr_frags_first);
	if (ret) {
		dev_kfree_skb_any(skb);
		adapter->stats.tx_errors++;
@@ -1439,9 +1522,8 @@ void edma_flow_may_expire(unsigned long data)
			res = rps_may_expire_flow(adapter->netdev, n->rq_id,
					n->flow_id, n->filter_id);
			if (res) {
-				int ret;
-				ret = edma_delete_rfs_filter(adapter, n);
-				if (ret < 0)
+				res = edma_delete_rfs_filter(adapter, n);
+				if (res < 0)
					dev_dbg(&adapter->netdev->dev,
							"RFS entry %d not allowed to be flushed by Switch",
							n->flow_id);
@@ -1463,7 +1545,7 @@ void edma_flow_may_expire(unsigned long data)
  *	Called by core to to steer the flow to CPU
  */
 int edma_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
-		       u16 rxq, u32 flow_id)
+		      u16 rxq, u32 flow_id)
 {
	struct flow_keys keys;
	struct edma_rfs_filter_node *filter_node;
@@ -1472,7 +1554,8 @@ int edma_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
	int res;

	if (skb->protocol == htons(ETH_P_IPV6)) {
-		dev_err(&adapter->pdev->dev, "IPv6 not supported\n");
+		if (net_ratelimit())
+			dev_err(&adapter->pdev->dev, "IPv6 not supported\n");
		res = -EINVAL;
		goto no_protocol_err;
	}
@@ -1549,6 +1632,7 @@ int edma_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
	return res;
 }

+#ifdef CONFIG_RFS_ACCEL
 /* edma_register_rfs_filter()
  *	Add RFS filter callback
  */
@@ -1569,6 +1653,19 @@ int edma_register_rfs_filter(struct net_device *netdev,

	return 0;
 }
+#endif
+
+/* edma_select_xps_queue()
+ *	Called by Linux TX stack to populate Linux TX queue
+ */
+u16 edma_select_xps_queue(struct net_device *dev, struct sk_buff *skb,
+				void *accel_priv, select_queue_fallback_t fallback)
+{
+	int cpu = get_cpu();
+	put_cpu();
+
+	return cpu;
+}

 /* edma_alloc_tx_rings()
  *	Allocate rx rings
@@ -1685,24 +1782,15 @@ void edma_free_queues(struct edma_common_info *edma_cinfo)
  */
 void edma_free_rx_resources(struct edma_common_info *edma_cinfo)
 {
-        struct edma_rfd_desc_ring *erdr;
-	struct edma_sw_desc *sw_desc;
+	struct edma_rfd_desc_ring *erdr;
	struct platform_device *pdev = edma_cinfo->pdev;
	int i, j, k;

	for (i = 0, k = 0; i < edma_cinfo->num_rx_queues; i++) {
		erdr = edma_cinfo->rfd_ring[k];
		for (j = 0; j < EDMA_RX_RING_SIZE; j++) {
-			sw_desc = &erdr->sw_desc[j];
-			if (likely(sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_HEAD)) {
-				dma_unmap_single(&pdev->dev, sw_desc->dma,
-					sw_desc->length, DMA_FROM_DEVICE);
-				edma_clean_rfd(erdr, j);
-			} else if ((sw_desc->flags & EDMA_SW_DESC_FLAG_SKB_FRAG)) {
-				dma_unmap_page(&pdev->dev, sw_desc->dma,
-					sw_desc->length, DMA_FROM_DEVICE);
-				edma_clean_rfd(erdr, j);
-			}
+			/* unmap all descriptors while cleaning */
+			edma_clean_rfd(pdev, erdr, j, 1);
		}
		k += ((edma_cinfo->num_rx_queues == 4) ? 2 : 1);

@@ -2115,7 +2203,7 @@ int edma_poll(struct napi_struct *napi, int budget)
	while (edma_percpu_info->rx_status) {
		queue_id = ffs(edma_percpu_info->rx_status) - 1;
		edma_rx_complete(edma_cinfo, &work_done,
-			        budget, queue_id, napi);
+				budget, queue_id, napi);

		if (likely(work_done < budget))
			edma_percpu_info->rx_status &= ~(1 << queue_id);
diff --git a/drivers/net/ethernet/qualcomm/essedma/edma.h b/drivers/net/ethernet/qualcomm/essedma/edma.h
index 5b0695b..7be5aab 100644
--- a/drivers/net/ethernet/qualcomm/essedma/edma.h
+++ b/drivers/net/ethernet/qualcomm/essedma/edma.h
@@ -59,12 +59,19 @@
 #define EDMA_LAN_DEFAULT_VLAN 1
 #define EDMA_WAN_DEFAULT_VLAN 2

-#define EDMA_DEFAULT_GROUP1_VLAN 1
-#define EDMA_DEFAULT_GROUP2_VLAN 2
+#define EDMA_DEFAULT_GROUP1_VLAN 2
+#define EDMA_DEFAULT_GROUP2_VLAN 1
 #define EDMA_DEFAULT_GROUP3_VLAN 3
 #define EDMA_DEFAULT_GROUP4_VLAN 4
 #define EDMA_DEFAULT_GROUP5_VLAN 5

+#define EDMA_DEFAULT_GROUP1_BMP 0x20
+#define EDMA_DEFAULT_GROUP2_BMP 0x1e
+
+#define EDMA_DEFAULT_DISABLE_RSS 0
+#define EDMA_RSS_DISABLE 1
+#define EDMA_RSS_ENABLE 0
+
 /* Queues exposed to linux kernel */
 #define EDMA_NETDEV_TX_QUEUE 4
 #define EDMA_NETDEV_RX_QUEUE 4
@@ -88,8 +95,8 @@

 /* TX/RX descriptor ring count */
 /* should be a power of 2 */
-#define EDMA_RX_RING_SIZE 128
-#define EDMA_TX_RING_SIZE 128
+#define EDMA_RX_RING_SIZE 512
+#define EDMA_TX_RING_SIZE 512

 /* Flags used in paged/non paged mode */
 #define EDMA_RX_HEAD_BUFF_SIZE_JUMBO 256
@@ -315,7 +322,7 @@ struct edma_common_info {
	struct ctl_table_header *edma_ctl_table_hdr;
	int num_gmac;
	struct edma_ethtool_statistics edma_ethstats; /* ethtool stats */
-	int num_rx_queues; /* number of rx queue */
+	u32 num_rx_queues; /* number of rx queue */
	u32 num_tx_queues; /* number of tx queue */
	u32 tx_irq[16]; /* number of tx irq */
	u32 rx_irq[8]; /* number of rx irq */
@@ -350,7 +357,7 @@ struct edma_tx_desc_ring {

 /* receive free descriptor (rfd) ring */
 struct edma_rfd_desc_ring {
-	void *hw_desc; /* descriptor ring virtual address */
+	struct edma_rx_free_desc *hw_desc; /* descriptor ring virtual address */
	struct edma_sw_desc *sw_desc; /* buffer associated with ring */
	u16 size; /* bytes allocated to sw_desc */
	u16 count; /* number of descriptors in the ring */
@@ -386,7 +393,9 @@ struct edma_adapter {
	struct phy_device *phydev; /* Phy device */
	struct edma_rfs_flow_table rfs; /* edma rfs flow table */
	struct net_device_stats stats; /* netdev statistics */
+#ifdef CONFIG_RFS_ACCEL
	set_rfs_filter_callback_t set_rfs_rule;
+#endif
	u32 flags;/* status flags */
	unsigned long state_flags; /* GMAC up/down flags */
	u32 forced_speed; /* link force speed */
@@ -394,6 +403,7 @@ struct edma_adapter {
	u32 link_state; /* phy link state */
	u32 phy_mdio_addr; /* PHY device address on MII interface */
	u32 poll_required; /* check if link polling is required */
+	u32 poll_required_dynamic; /* dynamic polling flag */
	u32 tx_start_offset[CONFIG_NR_CPUS]; /* tx queue start */
	u32 default_vlan_tag; /* vlan tag */
	u32 dp_bitmap;
@@ -429,8 +439,10 @@ struct net_device_stats *edma_get_stats(struct net_device *netdev);
 int edma_set_mac_addr(struct net_device *netdev, void *p);
 int edma_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
		u16 rxq, u32 flow_id);
+#ifdef CONFIG_RFS_ACCEL
 int edma_register_rfs_filter(struct net_device *netdev,
		set_rfs_filter_callback_t set_filter);
+#endif
 void edma_flow_may_expire(unsigned long data);
 void edma_set_ethtool_ops(struct net_device *netdev);
 int edma_change_mtu(struct net_device *netdev, int new_mtu);
@@ -439,6 +451,8 @@ void edma_assign_ath_hdr_type(int tag);
 int edma_get_default_vlan_tag(struct net_device *netdev);
 void edma_adjust_link(struct net_device *netdev);
 int edma_fill_netdev(struct edma_common_info *edma_cinfo, int qid, int num, int txq_id);
+u16 edma_select_xps_queue(struct net_device *dev, struct sk_buff *skb,
+	void *accel_priv, select_queue_fallback_t fallback);
 void edma_read_append_stats(struct edma_common_info *edma_cinfo);
 void edma_change_tx_coalesce(int usecs);
 void edma_change_rx_coalesce(int usecs);
diff --git a/drivers/net/ethernet/qualcomm/essedma/edma_axi.c b/drivers/net/ethernet/qualcomm/essedma/edma_axi.c
index 94ff340..6fe9019 100644
--- a/drivers/net/ethernet/qualcomm/essedma/edma_axi.c
+++ b/drivers/net/ethernet/qualcomm/essedma/edma_axi.c
@@ -853,6 +1263,13 @@ static int edma_axi_probe(struct platform_device *pdev)
		edma_netdev[i]->vlan_features |= NETIF_F_RXHASH | NETIF_F_NTUPLE;
		edma_netdev[i]->wanted_features |= NETIF_F_RXHASH | NETIF_F_NTUPLE;
 #endif
+		if (edma_cinfo->fraglist_mode) {
+			edma_netdev[i]->features |= NETIF_F_FRAGLIST;
+			edma_netdev[i]->hw_features |= NETIF_F_FRAGLIST;
+			edma_netdev[i]->vlan_features |= NETIF_F_FRAGLIST;
+			edma_netdev[i]->wanted_features |= NETIF_F_FRAGLIST;
+		}
+
		edma_set_ethtool_ops(edma_netdev[i]);

		/* This just fill in some default MAC address
@@ -927,7 +1344,11 @@ static int edma_axi_probe(struct platform_device *pdev)
			portid_bmp &= ~(1 << (port_bit - 1));
		}

-		if (!of_property_read_u32(pnp, "qcom,poll-required",
+		if (of_property_read_u32(pnp, "qcom,poll-required-dynamic",
+					 &adapter[idx]->poll_required_dynamic))
+			adapter[idx]->poll_required_dynamic = 0;
+
+		if (!of_property_read_u32(pnp, "qcom,poll-required",
					  &adapter[idx]->poll_required)) {
			if (adapter[idx]->poll_required) {
				of_property_read_u32(pnp, "qcom,phy-mdio-addr",
diff --git a/drivers/net/ethernet/qualcomm/essedma/edma_ethtool.c b/drivers/net/ethernet/qualcomm/essedma/edma_ethtool.c
index 377dc3e..423f5f0 100644
--- a/drivers/net/ethernet/qualcomm/essedma/edma_ethtool.c
+++ b/drivers/net/ethernet/qualcomm/essedma/edma_ethtool.c
@@ -184,7 +184,30 @@ static int edma_get_settings(struct net_device *netdev,
 {
	struct edma_adapter *adapter = netdev_priv(netdev);

-	if (adapter->poll_required) {
+	if (!(adapter->poll_required)) {
+		/* If the speed/duplex for this GMAC is forced and we
+		 * are not polling for link state changes, return the
+		 * values as specified by platform. This will be true
+		 * for GMACs connected to switch, and interfaces that
+		 * do not use a PHY.
+		 */
+		if (adapter->forced_speed != SPEED_UNKNOWN) {
+			/* set speed and duplex */
+			ethtool_cmd_speed_set(ecmd, SPEED_1000);
+			ecmd->duplex = DUPLEX_FULL;
+
+			/* Populate capabilities advertised by self */
+			ecmd->advertising = 0;
+			ecmd->autoneg = 0;
+			ecmd->port = PORT_TP;
+			ecmd->transceiver = XCVR_EXTERNAL;
+		} else {
+			/* non link polled and non
+			 * forced speed/duplex interface
+			 */
+			return -EIO;
+		}
+	} else {
		struct phy_device *phydev = NULL;
		uint16_t phyreg;

@@ -226,31 +249,6 @@ static int edma_get_settings(struct net_device *netdev,

		if (phyreg & LPA_1000FULL)
			ecmd->lp_advertising |= ADVERTISED_1000baseT_Full;
-	} else {
-		/* If the speed/duplex for this GMAC is forced and we
-		 * are not polling for link state changes, return the
-		 * values as specified by platform. This will be true
-		 * for GMACs connected to switch, and interfaces that
-		 * do not use a PHY.
-		 */
-		if (!(adapter->poll_required)) {
-			if (adapter->forced_speed != SPEED_UNKNOWN) {
-				/* set speed and duplex */
-				ethtool_cmd_speed_set(ecmd, SPEED_1000);
-				ecmd->duplex = DUPLEX_FULL;
-
-				/* Populate capabilities advertised by self */
-				ecmd->advertising = 0;
-				ecmd->autoneg = 0;
-				ecmd->port = PORT_TP;
-				ecmd->transceiver = XCVR_EXTERNAL;
-			} else {
-				/* non link polled and non
-				 * forced speed/duplex interface
-				 */
-				return -EIO;
-			}
-		}
	}

	return 0;
diff --git a/drivers/net/ethernet/qualcomm/essedma/ess_edma.h b/drivers/net/ethernet/qualcomm/essedma/ess_edma.h
index 81a5ddd..dd76322 100644
--- a/drivers/net/ethernet/qualcomm/essedma/ess_edma.h
+++ b/drivers/net/ethernet/qualcomm/essedma/ess_edma.h
@@ -317,6 +317,8 @@ struct edma_hw;

 /* RRD descriptor fields */
 #define EDMA_RRD_NUM_RFD_MASK 0x000F
+#define EDMA_RRD_PKT_SIZE_MASK 0x3FFF
+#define EDMA_RRD_SRC_PORT_NUM_MASK 0x4000
 #define EDMA_RRD_SVLAN 0x8000
 #define EDMA_RRD_FLOW_COOKIE_MASK 0x07FF;

--
2.7.2
